{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import gym\n",
    "import gnwrapper\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, Dropout, GlobalAveragePooling2D, Permute, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = 'Pong-v0'\n",
    "WINDOW_LENGTH = 4\n",
    "INPUT_SHAPE = (160, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "np.random.seed(123)\n",
    "env.seed(123)\n",
    "nb_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        assert observation.ndim == 3  # (height, width, channel)\n",
    "        #observation = cv2.resize(observation, INPUT_SHAPE)\n",
    "        #processed_observation = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
    "        img = Image.fromarray(observation)\n",
    "        img = img.resize(INPUT_SHAPE).convert('L')  # resize and convert to grayscale\n",
    "        processed_observation = np.array(img)\n",
    "        assert processed_observation.shape == INPUT_SHAPE\n",
    "        return processed_observation.astype('uint8')  # saves storage in experience memory\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        # We could perform this processing step in `process_observation`. In this case, however,\n",
    "        # we would need to store a `float32` array instead, which is 4x more memory intensive than\n",
    "        # an `uint8` array. This matters if we store 1M observations.\n",
    "        processed_batch = batch.astype('float32') / 255.\n",
    "        return processed_batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "permute (Permute)            (None, 160, 160, 4)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 153, 153, 32)      8224      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 76, 76, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 73, 73, 64)        32832     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 390       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 123,558\n",
      "Trainable params: 123,558\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "input_shape=(WINDOW_LENGTH,) + INPUT_SHAPE\n",
    "model = Sequential()\n",
    "model.add(Permute((2, 3, 1), input_shape=input_shape))\n",
    "model.add(Conv2D(32, (8, 8), activation=\"relu\"))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(64, (4, 4), activation=\"relu\"))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(128, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=1000000, window_length=WINDOW_LENGTH)\n",
    "policy = LinearAnnealedPolicy(\n",
    "    EpsGreedyQPolicy(),\n",
    "    attr='eps',\n",
    "    value_max=1.,\n",
    "    value_min=.1,\n",
    "    value_test=.05,\n",
    "    nb_steps=1000000\n",
    ")\n",
    "dqn = DQNAgent(\n",
    "    model=model, \n",
    "    nb_actions=nb_actions,\n",
    "    memory=memory,\n",
    "    policy=policy,\n",
    "    processor = AtariProcessor(),\n",
    "    nb_steps_warmup=50000,\n",
    "    target_model_update=10000,\n",
    "    train_interval=4\n",
    ")\n",
    "dqn.compile(Adam(learning_rate=.00025), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 175000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/python/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1260/175000: episode: 1, duration: 20.095s, episode steps: 1260, steps per second:  63, episode reward: -19.000, mean reward: -0.015 [-1.000,  1.000], mean action: 2.444 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   2436/175000: episode: 2, duration: 6.174s, episode steps: 1176, steps per second: 190, episode reward: -21.000, mean reward: -0.018 [-1.000,  0.000], mean action: 2.384 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   3667/175000: episode: 3, duration: 6.374s, episode steps: 1231, steps per second: 193, episode reward: -20.000, mean reward: -0.016 [-1.000,  1.000], mean action: 2.480 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   4800/175000: episode: 4, duration: 5.885s, episode steps: 1133, steps per second: 193, episode reward: -21.000, mean reward: -0.019 [-1.000,  0.000], mean action: 2.487 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   6091/175000: episode: 5, duration: 6.693s, episode steps: 1291, steps per second: 193, episode reward: -20.000, mean reward: -0.015 [-1.000,  1.000], mean action: 2.461 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   7435/175000: episode: 6, duration: 6.901s, episode steps: 1344, steps per second: 195, episode reward: -21.000, mean reward: -0.016 [-1.000,  0.000], mean action: 2.425 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   8577/175000: episode: 7, duration: 5.969s, episode steps: 1142, steps per second: 191, episode reward: -21.000, mean reward: -0.018 [-1.000,  0.000], mean action: 2.511 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "   9662/175000: episode: 8, duration: 5.626s, episode steps: 1085, steps per second: 193, episode reward: -21.000, mean reward: -0.019 [-1.000,  0.000], mean action: 2.512 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  10890/175000: episode: 9, duration: 6.251s, episode steps: 1228, steps per second: 196, episode reward: -21.000, mean reward: -0.017 [-1.000,  0.000], mean action: 2.431 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  12085/175000: episode: 10, duration: 6.171s, episode steps: 1195, steps per second: 194, episode reward: -21.000, mean reward: -0.018 [-1.000,  0.000], mean action: 2.411 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  13296/175000: episode: 11, duration: 6.358s, episode steps: 1211, steps per second: 190, episode reward: -20.000, mean reward: -0.017 [-1.000,  1.000], mean action: 2.502 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  14359/175000: episode: 12, duration: 5.610s, episode steps: 1063, steps per second: 189, episode reward: -21.000, mean reward: -0.020 [-1.000,  0.000], mean action: 2.589 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  15452/175000: episode: 13, duration: 5.621s, episode steps: 1093, steps per second: 194, episode reward: -21.000, mean reward: -0.019 [-1.000,  0.000], mean action: 2.470 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  16508/175000: episode: 14, duration: 5.426s, episode steps: 1056, steps per second: 195, episode reward: -21.000, mean reward: -0.020 [-1.000,  0.000], mean action: 2.437 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  17712/175000: episode: 15, duration: 6.153s, episode steps: 1204, steps per second: 196, episode reward: -20.000, mean reward: -0.017 [-1.000,  1.000], mean action: 2.549 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  18811/175000: episode: 16, duration: 5.660s, episode steps: 1099, steps per second: 194, episode reward: -21.000, mean reward: -0.019 [-1.000,  0.000], mean action: 2.459 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  20135/175000: episode: 17, duration: 6.904s, episode steps: 1324, steps per second: 192, episode reward: -21.000, mean reward: -0.016 [-1.000,  0.000], mean action: 2.405 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  21240/175000: episode: 18, duration: 5.663s, episode steps: 1105, steps per second: 195, episode reward: -21.000, mean reward: -0.019 [-1.000,  0.000], mean action: 2.397 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  22460/175000: episode: 19, duration: 6.296s, episode steps: 1220, steps per second: 194, episode reward: -21.000, mean reward: -0.017 [-1.000,  0.000], mean action: 2.433 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  23584/175000: episode: 20, duration: 5.840s, episode steps: 1124, steps per second: 192, episode reward: -21.000, mean reward: -0.019 [-1.000,  0.000], mean action: 2.401 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  24691/175000: episode: 21, duration: 5.839s, episode steps: 1107, steps per second: 190, episode reward: -21.000, mean reward: -0.019 [-1.000,  0.000], mean action: 2.479 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  25907/175000: episode: 22, duration: 6.261s, episode steps: 1216, steps per second: 194, episode reward: -21.000, mean reward: -0.017 [-1.000,  0.000], mean action: 2.461 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  27195/175000: episode: 23, duration: 6.781s, episode steps: 1288, steps per second: 190, episode reward: -21.000, mean reward: -0.016 [-1.000,  0.000], mean action: 2.440 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  28510/175000: episode: 24, duration: 6.827s, episode steps: 1315, steps per second: 193, episode reward: -21.000, mean reward: -0.016 [-1.000,  0.000], mean action: 2.484 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  29532/175000: episode: 25, duration: 5.323s, episode steps: 1022, steps per second: 192, episode reward: -21.000, mean reward: -0.021 [-1.000,  0.000], mean action: 2.448 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  30738/175000: episode: 26, duration: 6.266s, episode steps: 1206, steps per second: 192, episode reward: -20.000, mean reward: -0.017 [-1.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  32171/175000: episode: 27, duration: 7.452s, episode steps: 1433, steps per second: 192, episode reward: -20.000, mean reward: -0.014 [-1.000,  1.000], mean action: 2.437 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  33519/175000: episode: 28, duration: 7.009s, episode steps: 1348, steps per second: 192, episode reward: -20.000, mean reward: -0.015 [-1.000,  1.000], mean action: 2.386 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  34859/175000: episode: 29, duration: 6.993s, episode steps: 1340, steps per second: 192, episode reward: -19.000, mean reward: -0.014 [-1.000,  1.000], mean action: 2.394 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  35878/175000: episode: 30, duration: 5.232s, episode steps: 1019, steps per second: 195, episode reward: -21.000, mean reward: -0.021 [-1.000,  0.000], mean action: 2.430 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  37115/175000: episode: 31, duration: 6.327s, episode steps: 1237, steps per second: 196, episode reward: -21.000, mean reward: -0.017 [-1.000,  0.000], mean action: 2.374 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  38357/175000: episode: 32, duration: 6.260s, episode steps: 1242, steps per second: 198, episode reward: -21.000, mean reward: -0.017 [-1.000,  0.000], mean action: 2.521 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  39409/175000: episode: 33, duration: 5.353s, episode steps: 1052, steps per second: 197, episode reward: -21.000, mean reward: -0.020 [-1.000,  0.000], mean action: 2.509 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  40515/175000: episode: 34, duration: 5.691s, episode steps: 1106, steps per second: 194, episode reward: -21.000, mean reward: -0.019 [-1.000,  0.000], mean action: 2.416 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  41904/175000: episode: 35, duration: 7.103s, episode steps: 1389, steps per second: 196, episode reward: -21.000, mean reward: -0.015 [-1.000,  0.000], mean action: 2.465 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  43272/175000: episode: 36, duration: 7.189s, episode steps: 1368, steps per second: 190, episode reward: -19.000, mean reward: -0.014 [-1.000,  1.000], mean action: 2.428 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  44440/175000: episode: 37, duration: 5.992s, episode steps: 1168, steps per second: 195, episode reward: -20.000, mean reward: -0.017 [-1.000,  1.000], mean action: 2.409 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  45566/175000: episode: 38, duration: 5.866s, episode steps: 1126, steps per second: 192, episode reward: -21.000, mean reward: -0.019 [-1.000,  0.000], mean action: 2.395 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  46863/175000: episode: 39, duration: 6.770s, episode steps: 1297, steps per second: 192, episode reward: -19.000, mean reward: -0.015 [-1.000,  1.000], mean action: 2.343 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  48005/175000: episode: 40, duration: 5.935s, episode steps: 1142, steps per second: 192, episode reward: -20.000, mean reward: -0.018 [-1.000,  1.000], mean action: 2.368 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n",
      "  49360/175000: episode: 41, duration: 6.919s, episode steps: 1355, steps per second: 196, episode reward: -20.000, mean reward: -0.015 [-1.000,  1.000], mean action: 2.390 [0.000, 5.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/python/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  50470/175000: episode: 42, duration: 87.361s, episode steps: 1110, steps per second:  13, episode reward: -21.000, mean reward: -0.019 [-1.000,  0.000], mean action: 2.476 [0.000, 5.000],  loss: 0.011216, mae: 0.117274, mean_q: 0.183859, mean_eps: 0.954788\n",
      "  51832/175000: episode: 43, duration: 73.480s, episode steps: 1362, steps per second:  19, episode reward: -20.000, mean reward: -0.015 [-1.000,  1.000], mean action: 2.468 [0.000, 5.000],  loss: 0.009063, mae: 0.124221, mean_q: 0.180612, mean_eps: 0.953965\n",
      "  52973/175000: episode: 44, duration: 62.662s, episode steps: 1141, steps per second:  18, episode reward: -21.000, mean reward: -0.018 [-1.000,  0.000], mean action: 2.499 [0.000, 5.000],  loss: 0.009931, mae: 0.125876, mean_q: 0.173087, mean_eps: 0.952838\n",
      "  54288/175000: episode: 45, duration: 72.008s, episode steps: 1315, steps per second:  18, episode reward: -21.000, mean reward: -0.016 [-1.000,  0.000], mean action: 2.560 [0.000, 5.000],  loss: 0.008656, mae: 0.126722, mean_q: 0.170265, mean_eps: 0.951733\n",
      "  55561/175000: episode: 46, duration: 68.102s, episode steps: 1273, steps per second:  19, episode reward: -19.000, mean reward: -0.015 [-1.000,  1.000], mean action: 2.605 [0.000, 5.000],  loss: 0.009706, mae: 0.127732, mean_q: 0.167437, mean_eps: 0.950568\n",
      "  56712/175000: episode: 47, duration: 62.766s, episode steps: 1151, steps per second:  18, episode reward: -20.000, mean reward: -0.017 [-1.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.008912, mae: 0.128350, mean_q: 0.166862, mean_eps: 0.949478\n",
      "  57802/175000: episode: 48, duration: 58.733s, episode steps: 1090, steps per second:  19, episode reward: -21.000, mean reward: -0.019 [-1.000,  0.000], mean action: 2.466 [0.000, 5.000],  loss: 0.009113, mae: 0.128647, mean_q: 0.165434, mean_eps: 0.948470\n",
      "  59294/175000: episode: 49, duration: 81.396s, episode steps: 1492, steps per second:  18, episode reward: -19.000, mean reward: -0.013 [-1.000,  1.000], mean action: 2.371 [0.000, 5.000],  loss: 0.008914, mae: 0.129083, mean_q: 0.164322, mean_eps: 0.947307\n",
      "  60345/175000: episode: 50, duration: 58.532s, episode steps: 1051, steps per second:  18, episode reward: -21.000, mean reward: -0.020 [-1.000,  0.000], mean action: 2.590 [0.000, 5.000],  loss: 0.009672, mae: 0.124573, mean_q: 0.156287, mean_eps: 0.946162\n",
      "  61427/175000: episode: 51, duration: 59.300s, episode steps: 1082, steps per second:  18, episode reward: -21.000, mean reward: -0.019 [-1.000,  0.000], mean action: 2.491 [0.000, 5.000],  loss: 0.009502, mae: 0.112610, mean_q: 0.138737, mean_eps: 0.945203\n",
      "  62789/175000: episode: 52, duration: 74.835s, episode steps: 1362, steps per second:  18, episode reward: -19.000, mean reward: -0.014 [-1.000,  1.000], mean action: 2.455 [0.000, 5.000],  loss: 0.009031, mae: 0.113237, mean_q: 0.139482, mean_eps: 0.944103\n",
      "  63894/175000: episode: 53, duration: 60.485s, episode steps: 1105, steps per second:  18, episode reward: -21.000, mean reward: -0.019 [-1.000,  0.000], mean action: 2.383 [0.000, 5.000],  loss: 0.009683, mae: 0.113336, mean_q: 0.138342, mean_eps: 0.942992\n",
      "  64982/175000: episode: 54, duration: 59.685s, episode steps: 1088, steps per second:  18, episode reward: -21.000, mean reward: -0.019 [-1.000,  0.000], mean action: 2.343 [0.000, 5.000],  loss: 0.009188, mae: 0.114418, mean_q: 0.139311, mean_eps: 0.942006\n",
      "  66258/175000: episode: 55, duration: 68.519s, episode steps: 1276, steps per second:  19, episode reward: -20.000, mean reward: -0.016 [-1.000,  1.000], mean action: 2.339 [0.000, 5.000],  loss: 0.008229, mae: 0.114160, mean_q: 0.139737, mean_eps: 0.940942\n",
      "  67305/175000: episode: 56, duration: 57.892s, episode steps: 1047, steps per second:  18, episode reward: -21.000, mean reward: -0.020 [-1.000,  0.000], mean action: 2.464 [0.000, 5.000],  loss: 0.008750, mae: 0.114939, mean_q: 0.139743, mean_eps: 0.939896\n",
      "  68670/175000: episode: 57, duration: 75.014s, episode steps: 1365, steps per second:  18, episode reward: -21.000, mean reward: -0.015 [-1.000,  0.000], mean action: 2.577 [0.000, 5.000],  loss: 0.009261, mae: 0.113887, mean_q: 0.137190, mean_eps: 0.938811\n",
      "  69929/175000: episode: 58, duration: 70.079s, episode steps: 1259, steps per second:  18, episode reward: -20.000, mean reward: -0.016 [-1.000,  1.000], mean action: 2.525 [0.000, 5.000],  loss: 0.009017, mae: 0.113850, mean_q: 0.137673, mean_eps: 0.937630\n",
      "  71451/175000: episode: 59, duration: 83.808s, episode steps: 1522, steps per second:  18, episode reward: -19.000, mean reward: -0.012 [-1.000,  1.000], mean action: 2.398 [0.000, 5.000],  loss: 0.008550, mae: 0.107912, mean_q: 0.129421, mean_eps: 0.936379\n",
      "  72543/175000: episode: 60, duration: 58.559s, episode steps: 1092, steps per second:  19, episode reward: -21.000, mean reward: -0.019 [-1.000,  0.000], mean action: 2.372 [0.000, 5.000],  loss: 0.010211, mae: 0.105553, mean_q: 0.125420, mean_eps: 0.935204\n",
      "  73818/175000: episode: 61, duration: 68.932s, episode steps: 1275, steps per second:  18, episode reward: -19.000, mean reward: -0.015 [-1.000,  1.000], mean action: 2.458 [0.000, 5.000],  loss: 0.010626, mae: 0.103996, mean_q: 0.122394, mean_eps: 0.934138\n",
      "  74870/175000: episode: 62, duration: 57.681s, episode steps: 1052, steps per second:  18, episode reward: -21.000, mean reward: -0.020 [-1.000,  0.000], mean action: 2.429 [0.000, 5.000],  loss: 0.008847, mae: 0.104329, mean_q: 0.122273, mean_eps: 0.933090\n",
      "  76427/175000: episode: 63, duration: 85.023s, episode steps: 1557, steps per second:  18, episode reward: -19.000, mean reward: -0.012 [-1.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.009300, mae: 0.106076, mean_q: 0.125445, mean_eps: 0.931917\n",
      "  77578/175000: episode: 64, duration: 63.505s, episode steps: 1151, steps per second:  18, episode reward: -20.000, mean reward: -0.017 [-1.000,  1.000], mean action: 2.679 [0.000, 5.000],  loss: 0.008463, mae: 0.106486, mean_q: 0.127079, mean_eps: 0.930698\n",
      "  78923/175000: episode: 65, duration: 74.626s, episode steps: 1345, steps per second:  18, episode reward: -20.000, mean reward: -0.015 [-1.000,  1.000], mean action: 2.603 [0.000, 5.000],  loss: 0.009006, mae: 0.106318, mean_q: 0.125505, mean_eps: 0.929575\n",
      "  80015/175000: episode: 66, duration: 59.570s, episode steps: 1092, steps per second:  18, episode reward: -21.000, mean reward: -0.019 [-1.000,  0.000], mean action: 2.545 [0.000, 5.000],  loss: 0.007450, mae: 0.106762, mean_q: 0.127378, mean_eps: 0.928479\n",
      "  81173/175000: episode: 67, duration: 63.090s, episode steps: 1158, steps per second:  18, episode reward: -21.000, mean reward: -0.018 [-1.000,  0.000], mean action: 2.419 [0.000, 5.000],  loss: 0.008556, mae: 0.098865, mean_q: 0.117649, mean_eps: 0.927465\n",
      "  82609/175000: episode: 68, duration: 79.396s, episode steps: 1436, steps per second:  18, episode reward: -19.000, mean reward: -0.013 [-1.000,  1.000], mean action: 2.537 [0.000, 5.000],  loss: 0.008518, mae: 0.097264, mean_q: 0.115434, mean_eps: 0.926297\n",
      "  83962/175000: episode: 69, duration: 74.015s, episode steps: 1353, steps per second:  18, episode reward: -20.000, mean reward: -0.015 [-1.000,  1.000], mean action: 2.648 [0.000, 5.000],  loss: 0.008646, mae: 0.096304, mean_q: 0.112057, mean_eps: 0.925043\n",
      "  85032/175000: episode: 70, duration: 59.832s, episode steps: 1070, steps per second:  18, episode reward: -21.000, mean reward: -0.020 [-1.000,  0.000], mean action: 2.557 [0.000, 5.000],  loss: 0.009749, mae: 0.095430, mean_q: 0.110456, mean_eps: 0.923954\n",
      "  86182/175000: episode: 71, duration: 63.971s, episode steps: 1150, steps per second:  18, episode reward: -20.000, mean reward: -0.017 [-1.000,  1.000], mean action: 2.536 [0.000, 5.000],  loss: 0.009040, mae: 0.096528, mean_q: 0.112568, mean_eps: 0.922955\n",
      "  87521/175000: episode: 72, duration: 72.375s, episode steps: 1339, steps per second:  19, episode reward: -21.000, mean reward: -0.016 [-1.000,  0.000], mean action: 2.463 [0.000, 5.000],  loss: 0.008853, mae: 0.096137, mean_q: 0.112412, mean_eps: 0.921833\n",
      "  88899/175000: episode: 73, duration: 76.139s, episode steps: 1378, steps per second:  18, episode reward: -20.000, mean reward: -0.015 [-1.000,  1.000], mean action: 2.441 [0.000, 5.000],  loss: 0.009789, mae: 0.095998, mean_q: 0.109991, mean_eps: 0.920611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  89929/175000: episode: 74, duration: 57.096s, episode steps: 1030, steps per second:  18, episode reward: -21.000, mean reward: -0.020 [-1.000,  0.000], mean action: 2.505 [0.000, 5.000],  loss: 0.008147, mae: 0.096128, mean_q: 0.111478, mean_eps: 0.919527\n",
      "  91019/175000: episode: 75, duration: 60.373s, episode steps: 1090, steps per second:  18, episode reward: -21.000, mean reward: -0.019 [-1.000,  0.000], mean action: 2.561 [0.000, 5.000],  loss: 0.009326, mae: 0.090593, mean_q: 0.107153, mean_eps: 0.918573\n",
      "  92108/175000: episode: 76, duration: 60.823s, episode steps: 1089, steps per second:  18, episode reward: -21.000, mean reward: -0.019 [-1.000,  0.000], mean action: 2.481 [0.000, 5.000],  loss: 0.008724, mae: 0.085757, mean_q: 0.100954, mean_eps: 0.917594\n",
      "  93598/175000: episode: 77, duration: 82.087s, episode steps: 1490, steps per second:  18, episode reward: -19.000, mean reward: -0.013 [-1.000,  1.000], mean action: 2.434 [0.000, 5.000],  loss: 0.009564, mae: 0.084318, mean_q: 0.098061, mean_eps: 0.916433\n",
      "  94804/175000: episode: 78, duration: 65.736s, episode steps: 1206, steps per second:  18, episode reward: -20.000, mean reward: -0.017 [-1.000,  1.000], mean action: 2.566 [0.000, 5.000],  loss: 0.009126, mae: 0.083627, mean_q: 0.097358, mean_eps: 0.915220\n",
      "  95815/175000: episode: 79, duration: 54.739s, episode steps: 1011, steps per second:  18, episode reward: -21.000, mean reward: -0.021 [-1.000,  0.000], mean action: 2.630 [0.000, 5.000],  loss: 0.007998, mae: 0.084109, mean_q: 0.097579, mean_eps: 0.914223\n",
      "  97080/175000: episode: 80, duration: 68.064s, episode steps: 1265, steps per second:  19, episode reward: -21.000, mean reward: -0.017 [-1.000,  0.000], mean action: 2.482 [0.000, 5.000],  loss: 0.007754, mae: 0.084309, mean_q: 0.098394, mean_eps: 0.913199\n",
      "  98382/175000: episode: 81, duration: 72.019s, episode steps: 1302, steps per second:  18, episode reward: -21.000, mean reward: -0.016 [-1.000,  0.000], mean action: 2.618 [0.000, 5.000],  loss: 0.007292, mae: 0.085674, mean_q: 0.099885, mean_eps: 0.912043\n",
      "  99463/175000: episode: 82, duration: 58.671s, episode steps: 1081, steps per second:  18, episode reward: -21.000, mean reward: -0.019 [-1.000,  0.000], mean action: 2.611 [0.000, 5.000],  loss: 0.009140, mae: 0.085384, mean_q: 0.099637, mean_eps: 0.910970\n",
      " 100705/175000: episode: 83, duration: 68.046s, episode steps: 1242, steps per second:  18, episode reward: -20.000, mean reward: -0.016 [-1.000,  1.000], mean action: 2.531 [0.000, 5.000],  loss: 0.008811, mae: 0.082028, mean_q: 0.097896, mean_eps: 0.909924\n",
      " 101879/175000: episode: 84, duration: 65.427s, episode steps: 1174, steps per second:  18, episode reward: -20.000, mean reward: -0.017 [-1.000,  1.000], mean action: 2.448 [0.000, 5.000],  loss: 0.008815, mae: 0.074565, mean_q: 0.086436, mean_eps: 0.908837\n",
      " 103101/175000: episode: 85, duration: 69.434s, episode steps: 1222, steps per second:  18, episode reward: -20.000, mean reward: -0.016 [-1.000,  1.000], mean action: 2.594 [0.000, 5.000],  loss: 0.008316, mae: 0.073038, mean_q: 0.086645, mean_eps: 0.907759\n",
      " 104418/175000: episode: 86, duration: 71.953s, episode steps: 1317, steps per second:  18, episode reward: -20.000, mean reward: -0.015 [-1.000,  1.000], mean action: 2.499 [0.000, 5.000],  loss: 0.008073, mae: 0.073546, mean_q: 0.086588, mean_eps: 0.906616\n",
      " 105666/175000: episode: 87, duration: 69.080s, episode steps: 1248, steps per second:  18, episode reward: -21.000, mean reward: -0.017 [-1.000,  0.000], mean action: 2.329 [0.000, 5.000],  loss: 0.009046, mae: 0.072973, mean_q: 0.085656, mean_eps: 0.905462\n",
      " 107031/175000: episode: 88, duration: 75.304s, episode steps: 1365, steps per second:  18, episode reward: -19.000, mean reward: -0.014 [-1.000,  1.000], mean action: 2.574 [0.000, 5.000],  loss: 0.008603, mae: 0.072647, mean_q: 0.084676, mean_eps: 0.904287\n",
      " 108320/175000: episode: 89, duration: 71.640s, episode steps: 1289, steps per second:  18, episode reward: -20.000, mean reward: -0.016 [-1.000,  1.000], mean action: 2.486 [0.000, 5.000],  loss: 0.009630, mae: 0.072747, mean_q: 0.083074, mean_eps: 0.903093\n",
      " 109534/175000: episode: 90, duration: 66.110s, episode steps: 1214, steps per second:  18, episode reward: -21.000, mean reward: -0.017 [-1.000,  0.000], mean action: 2.545 [0.000, 5.000],  loss: 0.008318, mae: 0.071844, mean_q: 0.082137, mean_eps: 0.901967\n",
      " 110907/175000: episode: 91, duration: 74.240s, episode steps: 1373, steps per second:  18, episode reward: -20.000, mean reward: -0.015 [-1.000,  1.000], mean action: 2.481 [0.000, 5.000],  loss: 0.008860, mae: 0.068573, mean_q: 0.079120, mean_eps: 0.900802\n",
      " 112064/175000: episode: 92, duration: 63.612s, episode steps: 1157, steps per second:  18, episode reward: -20.000, mean reward: -0.017 [-1.000,  1.000], mean action: 2.275 [0.000, 5.000],  loss: 0.009322, mae: 0.061622, mean_q: 0.070224, mean_eps: 0.899664\n",
      " 113158/175000: episode: 93, duration: 61.357s, episode steps: 1094, steps per second:  18, episode reward: -21.000, mean reward: -0.019 [-1.000,  0.000], mean action: 2.300 [0.000, 5.000],  loss: 0.008271, mae: 0.059559, mean_q: 0.069637, mean_eps: 0.898651\n",
      " 114452/175000: episode: 94, duration: 70.805s, episode steps: 1294, steps per second:  18, episode reward: -20.000, mean reward: -0.015 [-1.000,  1.000], mean action: 2.378 [0.000, 5.000],  loss: 0.008797, mae: 0.059431, mean_q: 0.069254, mean_eps: 0.897576\n",
      " 115601/175000: episode: 95, duration: 63.720s, episode steps: 1149, steps per second:  18, episode reward: -21.000, mean reward: -0.018 [-1.000,  0.000], mean action: 2.406 [0.000, 5.000],  loss: 0.008631, mae: 0.059721, mean_q: 0.070595, mean_eps: 0.896477\n",
      " 116857/175000: episode: 96, duration: 68.940s, episode steps: 1256, steps per second:  18, episode reward: -20.000, mean reward: -0.016 [-1.000,  1.000], mean action: 2.521 [0.000, 5.000],  loss: 0.008867, mae: 0.059422, mean_q: 0.068038, mean_eps: 0.895393\n",
      " 118002/175000: episode: 97, duration: 64.370s, episode steps: 1145, steps per second:  18, episode reward: -20.000, mean reward: -0.017 [-1.000,  1.000], mean action: 2.254 [0.000, 5.000],  loss: 0.008892, mae: 0.058816, mean_q: 0.066261, mean_eps: 0.894313\n",
      " 119166/175000: episode: 98, duration: 64.796s, episode steps: 1164, steps per second:  18, episode reward: -21.000, mean reward: -0.018 [-1.000,  0.000], mean action: 2.372 [0.000, 5.000],  loss: 0.008475, mae: 0.059241, mean_q: 0.068804, mean_eps: 0.893274\n",
      " 120281/175000: episode: 99, duration: 60.409s, episode steps: 1115, steps per second:  18, episode reward: -21.000, mean reward: -0.019 [-1.000,  0.000], mean action: 2.561 [0.000, 5.000],  loss: 0.008398, mae: 0.058517, mean_q: 0.067229, mean_eps: 0.892248\n",
      " 121329/175000: episode: 100, duration: 57.242s, episode steps: 1048, steps per second:  18, episode reward: -21.000, mean reward: -0.020 [-1.000,  0.000], mean action: 2.640 [0.000, 5.000],  loss: 0.008037, mae: 0.052284, mean_q: 0.059988, mean_eps: 0.891275\n",
      " 122549/175000: episode: 101, duration: 67.263s, episode steps: 1220, steps per second:  18, episode reward: -20.000, mean reward: -0.016 [-1.000,  1.000], mean action: 2.555 [0.000, 5.000],  loss: 0.008415, mae: 0.048958, mean_q: 0.056075, mean_eps: 0.890254\n",
      " 124136/175000: episode: 102, duration: 88.195s, episode steps: 1587, steps per second:  18, episode reward: -18.000, mean reward: -0.011 [-1.000,  1.000], mean action: 2.694 [0.000, 5.000],  loss: 0.008676, mae: 0.047454, mean_q: 0.053385, mean_eps: 0.888992\n",
      " 125309/175000: episode: 103, duration: 64.255s, episode steps: 1173, steps per second:  18, episode reward: -21.000, mean reward: -0.018 [-1.000,  0.000], mean action: 2.580 [0.000, 5.000],  loss: 0.009067, mae: 0.047298, mean_q: 0.054353, mean_eps: 0.887750\n",
      " 126758/175000: episode: 104, duration: 80.719s, episode steps: 1449, steps per second:  18, episode reward: -21.000, mean reward: -0.014 [-1.000,  0.000], mean action: 2.443 [0.000, 5.000],  loss: 0.007707, mae: 0.047190, mean_q: 0.054075, mean_eps: 0.886569\n",
      " 128045/175000: episode: 105, duration: 72.888s, episode steps: 1287, steps per second:  18, episode reward: -21.000, mean reward: -0.016 [-1.000,  0.000], mean action: 2.291 [0.000, 5.000],  loss: 0.008655, mae: 0.047824, mean_q: 0.052895, mean_eps: 0.885338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 129109/175000: episode: 106, duration: 59.435s, episode steps: 1064, steps per second:  18, episode reward: -21.000, mean reward: -0.020 [-1.000,  0.000], mean action: 2.410 [0.000, 5.000],  loss: 0.007996, mae: 0.047372, mean_q: 0.052534, mean_eps: 0.884280\n",
      " 130147/175000: episode: 107, duration: 57.235s, episode steps: 1038, steps per second:  18, episode reward: -21.000, mean reward: -0.020 [-1.000,  0.000], mean action: 2.566 [0.000, 5.000],  loss: 0.007790, mae: 0.047339, mean_q: 0.054362, mean_eps: 0.883335\n",
      " 131162/175000: episode: 108, duration: 55.711s, episode steps: 1015, steps per second:  18, episode reward: -21.000, mean reward: -0.021 [-1.000,  0.000], mean action: 2.699 [0.000, 5.000],  loss: 0.009001, mae: 0.042078, mean_q: 0.049796, mean_eps: 0.882411\n",
      " 132614/175000: episode: 109, duration: 80.146s, episode steps: 1452, steps per second:  18, episode reward: -19.000, mean reward: -0.013 [-1.000,  1.000], mean action: 2.443 [0.000, 5.000],  loss: 0.008829, mae: 0.037612, mean_q: 0.042086, mean_eps: 0.881301\n",
      " 134054/175000: episode: 110, duration: 78.649s, episode steps: 1440, steps per second:  18, episode reward: -19.000, mean reward: -0.013 [-1.000,  1.000], mean action: 2.588 [0.000, 5.000],  loss: 0.009591, mae: 0.036775, mean_q: 0.040093, mean_eps: 0.879999\n",
      " 135215/175000: episode: 111, duration: 63.297s, episode steps: 1161, steps per second:  18, episode reward: -21.000, mean reward: -0.018 [-1.000,  0.000], mean action: 2.584 [0.000, 5.000],  loss: 0.008019, mae: 0.036961, mean_q: 0.041016, mean_eps: 0.878829\n",
      " 136765/175000: episode: 112, duration: 86.570s, episode steps: 1550, steps per second:  18, episode reward: -18.000, mean reward: -0.012 [-1.000,  1.000], mean action: 2.652 [0.000, 5.000],  loss: 0.009248, mae: 0.037004, mean_q: 0.041118, mean_eps: 0.877609\n",
      " 138015/175000: episode: 113, duration: 68.084s, episode steps: 1250, steps per second:  18, episode reward: -20.000, mean reward: -0.016 [-1.000,  1.000], mean action: 2.658 [0.000, 5.000],  loss: 0.008964, mae: 0.036342, mean_q: 0.039898, mean_eps: 0.876349\n",
      " 139572/175000: episode: 114, duration: 86.551s, episode steps: 1557, steps per second:  18, episode reward: -19.000, mean reward: -0.012 [-1.000,  1.000], mean action: 2.435 [0.000, 5.000],  loss: 0.008008, mae: 0.036903, mean_q: 0.039826, mean_eps: 0.875087\n",
      " 140854/175000: episode: 115, duration: 69.974s, episode steps: 1282, steps per second:  18, episode reward: -19.000, mean reward: -0.015 [-1.000,  1.000], mean action: 2.257 [0.000, 5.000],  loss: 0.009170, mae: 0.032943, mean_q: 0.037069, mean_eps: 0.873809\n",
      " 141874/175000: episode: 116, duration: 56.719s, episode steps: 1020, steps per second:  18, episode reward: -21.000, mean reward: -0.021 [-1.000,  0.000], mean action: 2.326 [0.000, 5.000],  loss: 0.009033, mae: 0.026380, mean_q: 0.030503, mean_eps: 0.872772\n",
      " 143074/175000: episode: 117, duration: 65.178s, episode steps: 1200, steps per second:  18, episode reward: -20.000, mean reward: -0.017 [-1.000,  1.000], mean action: 2.474 [0.000, 5.000],  loss: 0.008272, mae: 0.024634, mean_q: 0.026179, mean_eps: 0.871773\n",
      " 144084/175000: episode: 118, duration: 56.494s, episode steps: 1010, steps per second:  18, episode reward: -21.000, mean reward: -0.021 [-1.000,  0.000], mean action: 2.614 [0.000, 5.000],  loss: 0.008155, mae: 0.025289, mean_q: 0.028793, mean_eps: 0.870780\n",
      " 145180/175000: episode: 119, duration: 60.427s, episode steps: 1096, steps per second:  18, episode reward: -21.000, mean reward: -0.019 [-1.000,  0.000], mean action: 2.505 [0.000, 5.000],  loss: 0.008307, mae: 0.024849, mean_q: 0.026920, mean_eps: 0.869833\n",
      " 146456/175000: episode: 120, duration: 71.520s, episode steps: 1276, steps per second:  18, episode reward: -20.000, mean reward: -0.016 [-1.000,  1.000], mean action: 2.577 [0.000, 5.000],  loss: 0.008458, mae: 0.023982, mean_q: 0.025999, mean_eps: 0.868766\n",
      " 147831/175000: episode: 121, duration: 75.217s, episode steps: 1375, steps per second:  18, episode reward: -19.000, mean reward: -0.014 [-1.000,  1.000], mean action: 2.636 [0.000, 5.000],  loss: 0.009326, mae: 0.024600, mean_q: 0.025520, mean_eps: 0.867572\n",
      " 148845/175000: episode: 122, duration: 56.799s, episode steps: 1014, steps per second:  18, episode reward: -21.000, mean reward: -0.021 [-1.000,  0.000], mean action: 2.505 [0.000, 5.000],  loss: 0.008827, mae: 0.023408, mean_q: 0.025861, mean_eps: 0.866496\n",
      " 149878/175000: episode: 123, duration: 56.357s, episode steps: 1033, steps per second:  18, episode reward: -21.000, mean reward: -0.020 [-1.000,  0.000], mean action: 2.573 [0.000, 5.000],  loss: 0.008562, mae: 0.023651, mean_q: 0.027209, mean_eps: 0.865574\n",
      " 150921/175000: episode: 124, duration: 58.843s, episode steps: 1043, steps per second:  18, episode reward: -21.000, mean reward: -0.020 [-1.000,  0.000], mean action: 2.385 [0.000, 5.000],  loss: 0.008476, mae: 0.020411, mean_q: 0.022387, mean_eps: 0.864640\n",
      " 152193/175000: episode: 125, duration: 71.059s, episode steps: 1272, steps per second:  18, episode reward: -19.000, mean reward: -0.015 [-1.000,  1.000], mean action: 2.356 [0.000, 5.000],  loss: 0.008098, mae: 0.016301, mean_q: 0.017402, mean_eps: 0.863598\n",
      " 153407/175000: episode: 126, duration: 67.109s, episode steps: 1214, steps per second:  18, episode reward: -20.000, mean reward: -0.016 [-1.000,  1.000], mean action: 2.349 [0.000, 5.000],  loss: 0.008714, mae: 0.014784, mean_q: 0.015620, mean_eps: 0.862480\n",
      " 154651/175000: episode: 127, duration: 69.891s, episode steps: 1244, steps per second:  18, episode reward: -20.000, mean reward: -0.016 [-1.000,  1.000], mean action: 2.279 [0.000, 5.000],  loss: 0.008295, mae: 0.014415, mean_q: 0.014082, mean_eps: 0.861375\n",
      " 155853/175000: episode: 128, duration: 67.407s, episode steps: 1202, steps per second:  18, episode reward: -21.000, mean reward: -0.017 [-1.000,  0.000], mean action: 2.369 [0.000, 5.000],  loss: 0.009312, mae: 0.014086, mean_q: 0.012705, mean_eps: 0.860273\n",
      " 156922/175000: episode: 129, duration: 59.911s, episode steps: 1069, steps per second:  18, episode reward: -21.000, mean reward: -0.020 [-1.000,  0.000], mean action: 2.360 [0.000, 5.000],  loss: 0.007968, mae: 0.014012, mean_q: 0.014913, mean_eps: 0.859251\n",
      " 158141/175000: episode: 130, duration: 68.847s, episode steps: 1219, steps per second:  18, episode reward: -20.000, mean reward: -0.016 [-1.000,  1.000], mean action: 2.469 [0.000, 5.000],  loss: 0.009637, mae: 0.014172, mean_q: 0.014060, mean_eps: 0.858221\n",
      " 159743/175000: episode: 131, duration: 88.177s, episode steps: 1602, steps per second:  18, episode reward: -20.000, mean reward: -0.012 [-1.000,  1.000], mean action: 2.491 [0.000, 5.000],  loss: 0.008221, mae: 0.014154, mean_q: 0.013630, mean_eps: 0.856952\n",
      " 160790/175000: episode: 132, duration: 58.587s, episode steps: 1047, steps per second:  18, episode reward: -21.000, mean reward: -0.020 [-1.000,  0.000], mean action: 2.234 [0.000, 5.000],  loss: 0.009269, mae: 0.010411, mean_q: 0.009744, mean_eps: 0.855761\n",
      " 161901/175000: episode: 133, duration: 61.086s, episode steps: 1111, steps per second:  18, episode reward: -20.000, mean reward: -0.018 [-1.000,  1.000], mean action: 2.229 [0.000, 5.000],  loss: 0.009738, mae: 0.007907, mean_q: 0.002005, mean_eps: 0.854789\n",
      " 163237/175000: episode: 134, duration: 73.926s, episode steps: 1336, steps per second:  18, episode reward: -20.000, mean reward: -0.015 [-1.000,  1.000], mean action: 2.422 [0.000, 5.000],  loss: 0.008527, mae: 0.009378, mean_q: -0.000493, mean_eps: 0.853687\n",
      " 164270/175000: episode: 135, duration: 57.734s, episode steps: 1033, steps per second:  18, episode reward: -21.000, mean reward: -0.020 [-1.000,  0.000], mean action: 2.406 [0.000, 5.000],  loss: 0.008709, mae: 0.008519, mean_q: 0.002077, mean_eps: 0.852621\n",
      " 165326/175000: episode: 136, duration: 59.717s, episode steps: 1056, steps per second:  18, episode reward: -21.000, mean reward: -0.020 [-1.000,  0.000], mean action: 2.191 [0.000, 5.000],  loss: 0.008461, mae: 0.008484, mean_q: 0.001775, mean_eps: 0.851682\n",
      " 166578/175000: episode: 137, duration: 68.945s, episode steps: 1252, steps per second:  18, episode reward: -20.000, mean reward: -0.016 [-1.000,  1.000], mean action: 2.225 [0.000, 5.000],  loss: 0.007598, mae: 0.007939, mean_q: 0.001633, mean_eps: 0.850643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 167902/175000: episode: 138, duration: 74.414s, episode steps: 1324, steps per second:  18, episode reward: -20.000, mean reward: -0.015 [-1.000,  1.000], mean action: 2.314 [0.000, 5.000],  loss: 0.009476, mae: 0.009116, mean_q: 0.000742, mean_eps: 0.849484\n",
      " 168912/175000: episode: 139, duration: 55.438s, episode steps: 1010, steps per second:  18, episode reward: -21.000, mean reward: -0.021 [-1.000,  0.000], mean action: 2.770 [0.000, 5.000],  loss: 0.008673, mae: 0.009295, mean_q: -0.000363, mean_eps: 0.848435\n",
      " 169928/175000: episode: 140, duration: 56.316s, episode steps: 1016, steps per second:  18, episode reward: -21.000, mean reward: -0.021 [-1.000,  0.000], mean action: 2.696 [0.000, 5.000],  loss: 0.007356, mae: 0.007728, mean_q: 0.001165, mean_eps: 0.847524\n",
      " 170934/175000: episode: 141, duration: 55.322s, episode steps: 1006, steps per second:  18, episode reward: -21.000, mean reward: -0.021 [-1.000,  0.000], mean action: 2.597 [0.000, 5.000],  loss: 0.008397, mae: 0.010215, mean_q: -0.003227, mean_eps: 0.846613\n",
      " 172052/175000: episode: 142, duration: 63.457s, episode steps: 1118, steps per second:  18, episode reward: -20.000, mean reward: -0.018 [-1.000,  1.000], mean action: 2.515 [0.000, 5.000],  loss: 0.008554, mae: 0.016176, mean_q: -0.009814, mean_eps: 0.845657\n",
      " 173333/175000: episode: 143, duration: 71.996s, episode steps: 1281, steps per second:  18, episode reward: -21.000, mean reward: -0.016 [-1.000,  0.000], mean action: 2.416 [0.000, 5.000],  loss: 0.009711, mae: 0.019711, mean_q: -0.014261, mean_eps: 0.844577\n",
      " 174334/175000: episode: 144, duration: 55.691s, episode steps: 1001, steps per second:  18, episode reward: -21.000, mean reward: -0.021 [-1.000,  0.000], mean action: 2.477 [0.000, 5.000],  loss: 0.008663, mae: 0.019663, mean_q: -0.014205, mean_eps: 0.843549\n",
      "done, took 7225.803 seconds\n",
      "CPU times: user 2h 50s, sys: 3min 24s, total: 2h 4min 15s\n",
      "Wall time: 2h 25s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f966dbae450>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dqn.fit(env, nb_steps=175000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('atari.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: reward: -21.000, steps: 1029\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f996d4b3f90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dqn.test(gnwrapper.Animation(env), nb_episodes=1, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
